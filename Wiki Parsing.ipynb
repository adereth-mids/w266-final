{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import FloatProgress, IntText\n",
    "from IPython.display import display\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Articles from Specific Categories\n",
    "\n",
    "This is my first attempt at processing the Wikipedia dump.  It streamingly parses the Wikipedia XML and processes any article with a category tag that contains the specified strings.  Currently, it just writes the body of the articles to both a single file and a category specific file.  It probably should also do the tokenization, but doesn't yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ArticleProcessor:\n",
    "    \n",
    "    def __init__(self, categories):\n",
    "        self.categories = categories        \n",
    "        self.global_matcher = re.compile(\"\\[\\[Category:[^\\]]*(\" + \n",
    "                                         \"|\".join(categories) + \n",
    "                                         \")[^\\]]*\", re.IGNORECASE)\n",
    "        self.category_matcher = {}\n",
    "        self.article_writer = {}\n",
    "        self.global_writer = open(\"data/all-articles\", \"w\")\n",
    "        for category in self.categories:\n",
    "            self.category_matcher[category] = re.compile(\"\\[\\[Category:[^\\]]*\" + \n",
    "                                                         category + \n",
    "                                                         \"[^\\]]*\", re.IGNORECASE)\n",
    "            self.article_writer[category] = open(\"data/\" + category + \"-articles\", \"w\")\n",
    "            \n",
    "    def is_article_of_interest(self, article_text):\n",
    "        return self.global_matcher.search(article_text)\n",
    "\n",
    "    def process_article(self, article_text):\n",
    "        self.global_writer.write(article_text)\n",
    "        self.global_writer.write(\"\\n\")\n",
    "        for category in self.categories:\n",
    "            if self.category_matcher[category].search(article_text):\n",
    "                self.article_writer[category].write(article_text)\n",
    "                self.article_writer[category].write(\"\\n\")\n",
    "    \n",
    "    def close_all(self):\n",
    "        self.global_writer.close()\n",
    "        for writer in self.article_writer.values():\n",
    "            writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d400b5563d145e8bc9973d3d331497b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c1ec1d3c62421eaf29cf924126f38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c65b3e106148d6a4193acf90e6f695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles: 17773690 Time: 5338.332955360413 seconds\n"
     ]
    }
   ],
   "source": [
    "p = ET.iterparse(\"data/enwiki-20170820-pages-articles.xml\", \n",
    "                 events=(\"start\", \"end\"))\n",
    "\n",
    "start = time.time()\n",
    "article_count = 0\n",
    "root = None\n",
    "f = FloatProgress(min=0, max=17773690)\n",
    "t = IntText(value=0, description=\"Articles\")\n",
    "m = IntText(value=0, description=\"Matching Articles\")\n",
    "display(t, m, f)\n",
    "\n",
    "processor = ArticleProcessor([\"sportspeople\",\n",
    "                              \"artists\",\n",
    "                              \"politicians\",\n",
    "                              \"military personnel\",\n",
    "                              \"scientist\",\n",
    "                              #sportmanager\n",
    "                              #cleric\n",
    "                              \"monarch\",\n",
    "                              \"Fictional\\ characters\",\n",
    "                              \"nobility\",\n",
    "                              \"criminals\",\n",
    "                              \"judges\"\n",
    "                              \n",
    "                             ])\n",
    "try:\n",
    "    \n",
    "    for event, elem in p:\n",
    "        if root == None:\n",
    "            root = elem\n",
    "        if event == \"end\" and elem.tag == '{http://www.mediawiki.org/xml/export-0.10/}text':\n",
    "            article_count += 1\n",
    "            if article_count % 1000 == 0:\n",
    "                f.value = article_count\n",
    "                t.value = article_count\n",
    "            if elem.text and processor.is_article_of_interest(elem.text):\n",
    "                m.value += 1 \n",
    "                processor.process_article(elem.text)\n",
    "            root.clear()\n",
    "finally:\n",
    "    processor.close_all()\n",
    "    print(\"Articles:\", article_count, \"Time:\", (time.time() - start), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extracting Articles with Gender Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki id</th>\n",
       "      <th>gender</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>307</td>\n",
       "      <td>MALE</td>\n",
       "      <td>Abraham Lincoln</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>339</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>Ayn Rand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>340</td>\n",
       "      <td>MALE</td>\n",
       "      <td>Alain Connes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>344</td>\n",
       "      <td>MALE</td>\n",
       "      <td>Allan Dwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>595</td>\n",
       "      <td>MALE</td>\n",
       "      <td>Andre Agassi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>628</td>\n",
       "      <td>MALE</td>\n",
       "      <td>Aldous Huxley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>676</td>\n",
       "      <td>MALE</td>\n",
       "      <td>Andrei Tarkovsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>700</td>\n",
       "      <td>MALE</td>\n",
       "      <td>Arthur Schopenhauer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>711</td>\n",
       "      <td>MALE</td>\n",
       "      <td>Albert Sidney Johnston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>736</td>\n",
       "      <td>MALE</td>\n",
       "      <td>Albert Einstein</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wiki id  gender                    name\n",
       "0      307    MALE         Abraham Lincoln\n",
       "1      339  FEMALE                Ayn Rand\n",
       "2      340    MALE            Alain Connes\n",
       "3      344    MALE              Allan Dwan\n",
       "4      595    MALE            Andre Agassi\n",
       "5      628    MALE           Aldous Huxley\n",
       "6      676    MALE        Andrei Tarkovsky\n",
       "7      700    MALE     Arthur Schopenhauer\n",
       "8      711    MALE  Albert Sidney Johnston\n",
       "9      736    MALE         Albert Einstein"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_label_table = pd.read_csv(\"data/wiki.genders.txt\", sep='\\t')\n",
    "gender_label_table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "862171"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_ids_with_gender = set([str(x) for x in gender_label_table[\"wiki id\"]])\n",
    "len(gender_label_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0b8ff04d3d4361a94da666546c232f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8b547b386842569ef87eb3976880b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c57fea5fb84351aae803ad65ec1a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles: 17773690 Time: 4292.169456958771 seconds\n"
     ]
    }
   ],
   "source": [
    "p = ET.iterparse(\"data/enwiki-20170820-pages-articles.xml\", \n",
    "                 events=(\"start\", \"end\"))\n",
    "\n",
    "start = time.time()\n",
    "article_count = 0\n",
    "root = None\n",
    "f = FloatProgress(min=0, max=17773690)\n",
    "t = IntText(value=0, description=\"Articles\")\n",
    "m = IntText(value=0, description=\"Matching Articles\")\n",
    "display(t, m, f)\n",
    "\n",
    "article = None\n",
    "id = None\n",
    "\n",
    "article_writer = open(\"data/gendered-labeled-articles\", \"w\")\n",
    "\n",
    "try:\n",
    "    is_current_article_labeled = False\n",
    "    for event, elem in p:\n",
    "        if root == None:\n",
    "            root = elem\n",
    "        if event == \"start\" and elem.tag == '{http://www.mediawiki.org/xml/export-0.10/}page':\n",
    "            id = None\n",
    "        if id == None and event == \"end\" and elem.tag == '{http://www.mediawiki.org/xml/export-0.10/}id':\n",
    "            is_current_article_labeled = (elem.text in wiki_ids_with_gender)\n",
    "            id = elem.text\n",
    "        if event == \"end\" and elem.tag == '{http://www.mediawiki.org/xml/export-0.10/}text':\n",
    "            article_count += 1\n",
    "            if article_count % 1000 == 0:\n",
    "                f.value = article_count\n",
    "                t.value = article_count\n",
    "            if is_current_article_labeled and elem.text:\n",
    "                m.value += 1 \n",
    "                article = elem.text\n",
    "                article_writer.write(id)\n",
    "                article_writer.write(' ')\n",
    "                article_writer.write(article.replace('\\n', ' '))\n",
    "                article_writer.write('\\n')\n",
    "            root.clear()\n",
    "finally:\n",
    "    article_writer.close()\n",
    "    print(\"Articles:\", article_count, \"Time:\", (time.time() - start), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this completes, we have a file where each line starts with an article ID and is followed by the full markdown text of the article.  For speed, I'm cleaning it up with this command line chain of seds:\n",
    "\n",
    "```\n",
    "cat gendered-labeled-articles | sed 's/[^a-zA-Z0-9]/ /g' | \\\n",
    "sed 's/\\s\\{2,\\}/ /g' | \\\n",
    "tr '[:upper:]' '[:lower:]' > \\\n",
    "gendered-labeled-articles.stripped\n",
    "```\n",
    "\n",
    "That takes about 17.5 minutes to run.  This reduces the file from 5.8GB to 5.1GB.\n",
    "\n",
    "# Creating a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444f7d7184d042ac8625456777963ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d826e0dc910542caabbbb5967b9e059c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles: 850399 Time: 404.5375759601593 seconds\n"
     ]
    }
   ],
   "source": [
    "fp = FloatProgress(min=0, max=850399)\n",
    "uw = IntText(value=0, description=\"Unique\")\n",
    "display(fp, uw)\n",
    "article_count = 0\n",
    "word_count = 0\n",
    "\n",
    "\n",
    "article_ids = set()\n",
    "vocab_mapping = {}\n",
    "\n",
    "start = time.time()\n",
    "try:\n",
    "    with open(\"data/gendered-labeled-articles.stripped\", 'r') as f:\n",
    "        for line in f:\n",
    "            article_count += 1\n",
    "            if article_count % 1000 == 0:\n",
    "                fp.value = article_count\n",
    "                uw.value = word_count\n",
    "            words = line.split()\n",
    "            article_ids.add(words[0])\n",
    "            for word in words[1:]:\n",
    "                val = vocab_mapping.get(word, -1)\n",
    "                if val == -1:\n",
    "                    word_count += 1\n",
    "                    vocab_mapping[word] = word_count\n",
    "                \n",
    "finally:        \n",
    "    print(\"Articles:\", article_count, \"Time:\", (time.time() - start), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7869208"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_mapping[\"death\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
